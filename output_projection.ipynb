{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "# torch.manual_seed(42)\n",
    "\n",
    "class TwoLayerLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    更高效的 LSTM 实现，处理时间序列数据\n",
    "    \"\"\"\n",
    "    def __init__(self, k, hidden_dim=32, lstm_hidden_dim=64, dropout_rate=0.0):\n",
    "        super(TwoLayerLSTM, self).__init__()\n",
    "        self.k = k\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm_hidden_dim = lstm_hidden_dim\n",
    "        \n",
    "        # LSTM 层\n",
    "        self.lstm = nn.LSTM(k, lstm_hidden_dim, batch_first=True)\n",
    "        \n",
    "        # 全连接层\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.BatchNorm1d(lstm_hidden_dim),\n",
    "            nn.Linear(lstm_hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate) if dropout_rate > 0 else nn.Identity(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, N, lookback, k)\n",
    "        batch_size, N, lookback, k = x.shape\n",
    "        \n",
    "        # 重塑为 LSTM 输入: (batch_size*N, lookback, k)\n",
    "        x_lstm = x.reshape(-1, lookback, k)\n",
    "        \n",
    "        # LSTM 处理\n",
    "        lstm_out, _ = self.lstm(x_lstm)\n",
    "        # 使用最后一个时间步的输出\n",
    "        lstm_final = lstm_out[:, -1, :]  # shape: (batch_size*N, lstm_hidden_dim)\n",
    "        \n",
    "        # 全连接层处理\n",
    "        output = self.fc_layers(lstm_final)  # shape: (batch_size*N, 1)\n",
    "        \n",
    "        # 重塑为最终输出\n",
    "        output = output.view(batch_size, N)  # shape: (batch_size, N)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "\n",
    "# 在导入任何 pyepo 模块之前，先修复 HAS_GUROBI\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "# 强制重新加载模块以确保修改生效\n",
    "if 'pyepo.model.grb.grbmodel' in sys.modules:\n",
    "    del sys.modules['pyepo.model.grb.grbmodel']\n",
    "if 'pyepo.model.grb' in sys.modules:\n",
    "    del sys.modules['pyepo.model.grb']\n",
    "\n",
    "# 导入并修复\n",
    "from pyepo.model.grb import grbmodel\n",
    "grbmodel.HAS_GUROBI = True\n",
    "\n",
    "# 现在导入基类\n",
    "from pyepo.model.grb.grbmodel import optGrbModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "\n",
    "from pyepo.model.grb.grbmodel import optGrbModel\n",
    "\n",
    "\n",
    "class MarketNeutralGrbModel(optGrbModel):\n",
    "    \"\"\"\n",
    "    Enhanced Market Neutral Portfolio Optimization Model with Turnover Constraints\n",
    "    \n",
    "    This model includes:\n",
    "    - Market neutral constraint (sum of weights = 1)  \n",
    "    - Risk factor constraint |risk_f' x| <= risk_abs\n",
    "    - Single asset position limits |x_i| <= single_abs\n",
    "    - L1 norm constraint ∑|x_i| <= l1_abs\n",
    "    - Quadratic risk constraint x' * cov_matrix * x <= sigma_abs\n",
    "    - Optional turnover constraint ∑|x_i - w_prev_i| <= turnover\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        N,\n",
    "        A,\n",
    "        b,\n",
    "        l,\n",
    "        u,\n",
    "        minimize=False,\n",
    "        risk_f=None,\n",
    "        risk_abs=None,\n",
    "        single_abs=None,\n",
    "        l1_abs=None,\n",
    "        cov_matrix=None,\n",
    "        sigma_abs=None,\n",
    "        turnover=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the Market Neutral Optimization Model\n",
    "        \n",
    "        Args:\n",
    "            N (int): Number of assets\n",
    "            A (np.ndarray): Equality constraint matrix (1 x N)\n",
    "            b (np.ndarray): Equality constraint RHS (1,)\n",
    "            l (np.ndarray): Lower bounds for variables (N,)\n",
    "            u (np.ndarray): Upper bounds for variables (N,)\n",
    "            minimize (bool): Whether to minimize (True) or maximize (False) objective\n",
    "            risk_f (np.ndarray): Risk factor vector (N,)\n",
    "            risk_abs (float): Risk factor constraint bound\n",
    "            single_abs (float): Single asset position limit\n",
    "            l1_abs (float): L1 norm constraint bound\n",
    "            cov_matrix (np.ndarray): Covariance matrix (N x N)\n",
    "            sigma_abs (float): Quadratic risk constraint bound\n",
    "            turnover (float): Maximum allowed turnover (optional)\n",
    "        \"\"\"\n",
    "        # 保存所有参数\n",
    "        self.N = N\n",
    "        self.A = A\n",
    "        self.b = b\n",
    "        self.l = l\n",
    "        self.u = u\n",
    "        self.minimize = minimize\n",
    "        self.risk_f = risk_f\n",
    "        self.risk_abs = risk_abs\n",
    "        self.single_abs = single_abs\n",
    "        self.l1_abs = l1_abs\n",
    "        self.cov_matrix = cov_matrix\n",
    "        self.sigma_abs = sigma_abs\n",
    "        self.turnover = turnover\n",
    "        \n",
    "        # 保存前期权重，默认为等权重（首次投资）\n",
    "        self.w_prev = np.ones(N) / N  # Equal weights instead of zeros\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    def setPrevWeights(self, w_prev):\n",
    "        \"\"\"\n",
    "        Set previous portfolio weights for turnover constraint\n",
    "        \n",
    "        Args:\n",
    "            w_prev (np.ndarray): Previous portfolio weights\n",
    "        \"\"\"\n",
    "        if len(w_prev) != self.N:\n",
    "            raise ValueError(\"Size of previous weights must match number of assets\")\n",
    "        \n",
    "        # Check if w_prev is a PyTorch tensor\n",
    "        if isinstance(w_prev, torch.Tensor):\n",
    "            w_prev = w_prev.detach().cpu().numpy()\n",
    "        else:\n",
    "            w_prev = np.asarray(w_prev, dtype=np.float32)\n",
    "            \n",
    "        self.w_prev = w_prev\n",
    "        \n",
    "        # Update turnover constraints if they exist\n",
    "        if self.turnover is not None:\n",
    "            self._updateTurnoverConstraints()\n",
    "\n",
    "    def _getModel(self):\n",
    "        \"\"\"\n",
    "        Build the Gurobi optimization model\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (model, variables)\n",
    "        \"\"\"\n",
    "        # 新建 Gurobi 模型\n",
    "        m = gp.Model()\n",
    "\n",
    "        # 添加原始变量 x[i]\n",
    "        x = m.addVars(\n",
    "            self.N,\n",
    "            lb={i: float(self.l[i]) for i in range(self.N)},\n",
    "            ub={i: float(self.u[i]) for i in range(self.N)},\n",
    "            name=\"x\"\n",
    "        )\n",
    "\n",
    "        # 用于 L1 约束的辅助变量 t[i]\n",
    "        t = m.addVars(self.N, lb=0.0, name=\"t\")\n",
    "        \n",
    "        # 用于换手率约束的辅助变量 s[i] (如果启用换手率约束)\n",
    "        if self.turnover is not None:\n",
    "            s = m.addVars(self.N, lb=0.0, name=\"s\")  # s[i] >= |x[i] - w_prev[i]|\n",
    "\n",
    "        # 设置优化方向\n",
    "        m.modelSense = GRB.MINIMIZE if self.minimize else GRB.MAXIMIZE\n",
    "\n",
    "        # ——— 1) 市场中性约束 sum A[0,i] * x[i] == b[0]\n",
    "        m.addConstr(\n",
    "            gp.quicksum(self.A[0, i] * x[i] for i in range(self.N)) == float(self.b[0]),\n",
    "            name=\"eq_sum\"\n",
    "        )\n",
    "\n",
    "        # ——— 2) 风险约束 |risk_f' x| <= risk_abs\n",
    "        if self.risk_f is not None and self.risk_abs is not None:\n",
    "            expr_r = gp.quicksum(self.risk_f[i] * x[i] for i in range(self.N))\n",
    "            m.addConstr(expr_r <= float(self.risk_abs), name=\"risk_ub\")\n",
    "            m.addConstr(expr_r >= -float(self.risk_abs), name=\"risk_lb\")\n",
    "\n",
    "        # ——— 3) 单项绝对值约束 |x_i| <= single_abs\n",
    "        if self.single_abs is not None:\n",
    "            for i in range(self.N):\n",
    "                m.addConstr(x[i] <=  float(self.single_abs),  name=f\"single_ub_{i}\")\n",
    "                m.addConstr(x[i] >= -float(self.single_abs), name=f\"single_lb_{i}\")\n",
    "\n",
    "        # ——— 4) L1 约束 ∑|x_i| <= l1_abs\n",
    "        #   实现方式： t[i] >=  x[i], t[i] >= -x[i], 然后 ∑ t[i] <= l1_abs\n",
    "        if self.l1_abs is not None:\n",
    "            for i in range(self.N):\n",
    "                m.addConstr(t[i] >=  x[i], name=f\"t_pos_{i}\")\n",
    "                m.addConstr(t[i] >= -x[i], name=f\"t_neg_{i}\")\n",
    "            m.addConstr(t.sum() <= float(self.l1_abs), name=\"l1_norm\")\n",
    "\n",
    "        # ——— 5) 二次型约束 x' * cov_matrix * x <= sigma_abs\n",
    "        if self.cov_matrix is not None and self.sigma_abs is not None:\n",
    "            x_vec = np.array([x[i] for i in range(self.N)])\n",
    "            expr_q = x_vec @ self.cov_matrix @ x_vec\n",
    "            m.addQConstr(expr_q <= float(self.sigma_abs), name=\"sigma_qc\")\n",
    "\n",
    "        # ——— 6) 换手率约束 ∑|x_i - w_prev_i| <= turnover (如果启用)\n",
    "        if self.turnover is not None:\n",
    "            for i in range(self.N):\n",
    "                # s[i] >= x[i] - w_prev[i]\n",
    "                m.addConstr(s[i] >= x[i] - float(self.w_prev[i]), name=f\"turnover_pos_{i}\")\n",
    "                # s[i] >= -(x[i] - w_prev[i]) = w_prev[i] - x[i]\n",
    "                m.addConstr(s[i] >= float(self.w_prev[i]) - x[i], name=f\"turnover_neg_{i}\")\n",
    "            \n",
    "            # 换手率总约束\n",
    "            m.addConstr(s.sum() <= float(self.turnover), name=\"turnover_total\")\n",
    "            \n",
    "            # 保存辅助变量以便后续更新\n",
    "            self._turnover_vars = s\n",
    "\n",
    "        return m, x\n",
    "    \n",
    "    def _updateTurnoverConstraints(self):\n",
    "        \"\"\"\n",
    "        Update turnover constraints when w_prev changes\n",
    "        This is called automatically when setPrevWeights is used\n",
    "        \"\"\"\n",
    "        if self.turnover is None or not hasattr(self, '_turnover_vars'):\n",
    "            return\n",
    "            \n",
    "        # 更新换手率约束的右侧值\n",
    "        for i in range(self.N):\n",
    "            # 查找并更新对应的约束\n",
    "            for constr in self._model.getConstrs():\n",
    "                if constr.ConstrName == f\"turnover_pos_{i}\":\n",
    "                    # s[i] >= x[i] - w_prev[i] 更新为新的 w_prev[i]\n",
    "                    # 需要移除旧约束并添加新约束\n",
    "                    self._model.remove(constr)\n",
    "                    self._model.addConstr(\n",
    "                        self._turnover_vars[i] >= self.x[i] - float(self.w_prev[i]), \n",
    "                        name=f\"turnover_pos_{i}\"\n",
    "                    )\n",
    "                elif constr.ConstrName == f\"turnover_neg_{i}\":\n",
    "                    # s[i] >= w_prev[i] - x[i] 更新为新的 w_prev[i] \n",
    "                    self._model.remove(constr)\n",
    "                    self._model.addConstr(\n",
    "                        self._turnover_vars[i] >= float(self.w_prev[i]) - self.x[i], \n",
    "                        name=f\"turnover_neg_{i}\"\n",
    "                    )\n",
    "        \n",
    "        # 更新模型\n",
    "        self._model.update()\n",
    "\n",
    "    def getInfo(self):\n",
    "        \"\"\"\n",
    "        Get model information for debugging\n",
    "        \n",
    "        Returns:\n",
    "            dict: Model information\n",
    "        \"\"\"\n",
    "        info = {\n",
    "            'num_assets': self.N,\n",
    "            'has_turnover': self.turnover is not None,\n",
    "            'turnover_limit': self.turnover,\n",
    "            'prev_weights': self.w_prev.tolist() if self.w_prev is not None else None,\n",
    "            'constraints': {\n",
    "                'risk_factor': self.risk_abs is not None,\n",
    "                'single_abs': self.single_abs is not None,\n",
    "                'l1_norm': self.l1_abs is not None,\n",
    "                'quadratic_risk': self.sigma_abs is not None\n",
    "            }\n",
    "        }\n",
    "        return info\n",
    "\n",
    "    def solveSequential(self, cost_vectors):\n",
    "        \"\"\"\n",
    "        Solve multiple periods sequentially with turnover constraints\n",
    "        \n",
    "        Args:\n",
    "            cost_vectors (np.ndarray): Cost vectors for each period (T x N)\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (solutions, objectives) where solutions is list of T solutions\n",
    "                   and objectives is list of T objective values\n",
    "        \"\"\"\n",
    "        if cost_vectors.ndim != 2:\n",
    "            raise ValueError(\"cost_vectors must be 2D array (T x N)\")\n",
    "            \n",
    "        T, N = cost_vectors.shape\n",
    "        if N != self.N:\n",
    "            raise ValueError(f\"Number of assets in cost_vectors ({N}) must match model ({self.N})\")\n",
    "        \n",
    "        solutions = []\n",
    "        objectives = []\n",
    "        \n",
    "        # Reset to equal weights for first period\n",
    "        self.w_prev = np.ones(self.N) / self.N\n",
    "        \n",
    "        for t in range(T):\n",
    "            # Set objective for this period\n",
    "            self.setObj(cost_vectors[t])\n",
    "            \n",
    "            # Solve optimization problem\n",
    "            sol, obj = self.solve()\n",
    "            solutions.append(sol)\n",
    "            objectives.append(obj)\n",
    "            \n",
    "            # Set current solution as previous weights for next period\n",
    "            if t < T - 1 and self.turnover is not None:\n",
    "                self.setPrevWeights(sol)\n",
    "        \n",
    "        return solutions, objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Example parameters\n",
    "N = 335\n",
    "A = np.ones((1, N))\n",
    "b = np.array([1.0])\n",
    "l = np.zeros(N)\n",
    "u = np.ones(N) * 1e6\n",
    "\n",
    "# Risk parameters\n",
    "risk_f = np.random.randn(N)\n",
    "risk_abs = 1.5\n",
    "single_abs = 0.1\n",
    "l1_abs = 1.0\n",
    "sigma_abs = 2.5\n",
    "turnover = 0.3  # 30% maximum turnover\n",
    "\n",
    "# Covariance matrix\n",
    "M = np.random.randn(N, N)\n",
    "cov_matrix = M.T @ M + np.eye(N) * 1e-3\n",
    "\n",
    "# Create model with turnover constraints\n",
    "market_neutral_model = MarketNeutralGrbModel(\n",
    "    N, A, b, l, u, minimize=False,\n",
    "    risk_f=risk_f, risk_abs=risk_abs,\n",
    "    single_abs=single_abs, l1_abs=l1_abs,\n",
    "    cov_matrix=cov_matrix, sigma_abs=sigma_abs,\n",
    "    turnover=None\n",
    ")\n",
    "\n",
    "# Create model with turnover constraints\n",
    "market_neutral_model_testing = MarketNeutralGrbModel(\n",
    "    N, A, b, l, u, minimize=False,\n",
    "    risk_f=risk_f, risk_abs=risk_abs,\n",
    "    single_abs=single_abs, l1_abs=l1_abs,\n",
    "    cov_matrix=cov_matrix, sigma_abs=sigma_abs,\n",
    "    turnover=turnover\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pyepo import EPO\n",
    "\n",
    "\n",
    "\n",
    "def sequential_regret(predmodel, optmodel, dataloader, verbose=False):\n",
    "    \"\"\"\n",
    "    Calculate sequential regret for multi-period optimization with turnover constraints\n",
    "    \n",
    "    This function properly handles the sequential dependency where each period's\n",
    "    optimal solution depends on the previous period's portfolio weights.\n",
    "    \n",
    "    Args:\n",
    "        predmodel (nn.Module): Neural network for cost prediction\n",
    "        optmodel (MarketNeutralGrbModel): Enhanced optimization model with turnover support\n",
    "        dataloader (DataLoader): PyTorch dataloader with sequential data\n",
    "        verbose (bool): Whether to print detailed progress\n",
    "        \n",
    "    Returns:\n",
    "        float: Normalized sequential regret loss\n",
    "    \"\"\"\n",
    "    if not isinstance(optmodel, MarketNeutralGrbModel):\n",
    "        raise TypeError(\"optmodel must be MarketNeutralGrbModel for sequential regret\")\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    predmodel.eval()\n",
    "    \n",
    "    total_regret = 0.0\n",
    "    total_optsum = 0.0\n",
    "    num_sequences = 0\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Calculating sequential regret...\")\n",
    "        dataloader = tqdm(dataloader, desc=\"Processing sequences\")\n",
    "    \n",
    "    for batch_data in dataloader:\n",
    "        x, c, w_true, z_true = batch_data\n",
    "        if next(predmodel.parameters()).is_cuda:\n",
    "            x, c, w_true, z_true = x.cuda(), c.cuda(), w_true.cuda(), z_true.cuda()\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if predmodel.training:\n",
    "                predmodel.eval()\n",
    "            pred_costs = predmodel(x).to(\"cpu\").detach().numpy()  # (batch_size, num_assets) \n",
    "            \n",
    "            # Get true costs and solutions for this sequence\n",
    "            true_costs = c.to(\"cpu\").detach().numpy()  # (batch_size, num_assets) \n",
    "            true_solutions = w_true.to(\"cpu\").detach().numpy()  # (batch_size, num_assets) \n",
    "            true_objectives = z_true.to(\"cpu\").detach().numpy()  # (batch_size, 1) \n",
    "            \n",
    "            # Calculate sequential regret for this sequence\n",
    "            seq_regret, seq_optsum = _calculate_sequence_regret(\n",
    "                pred_costs, true_costs, true_solutions, true_objectives, optmodel\n",
    "            )\n",
    "            \n",
    "            total_regret += seq_regret\n",
    "            total_optsum += seq_optsum\n",
    "            num_sequences += 1\n",
    "    \n",
    "    # Turn back to train mode\n",
    "    predmodel.train()\n",
    "    \n",
    "    # Normalized regret\n",
    "    normalized_regret = total_regret / (total_optsum + 1e-7)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Processed {num_sequences} sequences\")\n",
    "        print(f\"Total regret: {total_regret:.6f}\")\n",
    "        print(f\"Total optimal sum: {total_optsum:.6f}\")\n",
    "        print(f\"Normalized regret: {normalized_regret:.6f}\")\n",
    "    \n",
    "    return normalized_regret\n",
    "\n",
    "\n",
    "def _calculate_sequence_regret(pred_costs, true_costs, true_solutions, true_objectives, optmodel):\n",
    "    \"\"\"\n",
    "    Calculate regret for a single sequence with sequential dependencies\n",
    "    \n",
    "    Args:\n",
    "        pred_costs (np.ndarray): Predicted costs (T, N)\n",
    "        true_costs (np.ndarray): True costs (T, N)\n",
    "        true_solutions (np.ndarray): True optimal solutions (T, N)\n",
    "        true_objectives (np.ndarray): True optimal objectives (T, 1)\n",
    "        optmodel (MarketNeutralGrbModel): Optimization model\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (sequence_regret, sequence_optsum)\n",
    "    \"\"\"\n",
    "    seq_len, num_assets = pred_costs.shape\n",
    "    sequence_regret = 0.0\n",
    "    sequence_optsum = 0.0\n",
    "    \n",
    "    # Reset model to equal weights for first period\n",
    "    optmodel.w_prev = np.ones(num_assets) / num_assets\n",
    "    \n",
    "    for t in range(seq_len):\n",
    "        try:\n",
    "            # Set predicted costs and solve\n",
    "            optmodel.setObj(pred_costs[t])\n",
    "            pred_solution, _ = optmodel.solve()\n",
    "            \n",
    "            # Convert to numpy if needed\n",
    "            if isinstance(pred_solution, torch.Tensor):\n",
    "                pred_solution = pred_solution.detach().cpu().numpy()\n",
    "            \n",
    "            # Calculate objective value using true costs\n",
    "            pred_obj_with_true_costs = np.dot(pred_solution, true_costs[t])\n",
    "            # Set predicted costs and solve\n",
    "            optmodel.setObj(true_costs[t])\n",
    "            _, true_obj = optmodel.solve()\n",
    "            \n",
    "            \n",
    "            # Calculate regret for this period\n",
    "            if optmodel.modelSense == EPO.MINIMIZE:\n",
    "                period_regret = pred_obj_with_true_costs - true_obj\n",
    "            elif optmodel.modelSense == EPO.MAXIMIZE:\n",
    "                period_regret = true_obj - pred_obj_with_true_costs\n",
    "            else:\n",
    "                raise ValueError(\"Invalid modelSense\")\n",
    "            \n",
    "            sequence_regret += period_regret\n",
    "            sequence_optsum += abs(true_obj)\n",
    "            \n",
    "            # Set current predicted solution as previous weights for next period\n",
    "            if t < seq_len - 1 and optmodel.turnover is not None:\n",
    "                optmodel.setPrevWeights(pred_solution)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in period {t}: {str(e)}\")\n",
    "            # Use true solution as fallback and continue\n",
    "            if optmodel.turnover is not None and t < seq_len - 1:\n",
    "                optmodel.setPrevWeights(true_solutions[t])\n",
    "    \n",
    "    return sequence_regret, sequence_optsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Precomputed mode enabled. Skipping time series processing and solution computation.\n"
     ]
    }
   ],
   "source": [
    "# 加载数据集\n",
    "\n",
    "from pyepo.data.dataset import optDataset\n",
    "import numpy as np\n",
    "\n",
    "def load_dataset_dict(filename):\n",
    "    \"\"\"从文件加载数据集字典\"\"\"\n",
    "    data = np.load(filename, allow_pickle=True)\n",
    "    return {\n",
    "        'feats': data['feats'],\n",
    "        'costs': data['costs'],\n",
    "        'sols': data['sols'],\n",
    "        'objs': data['objs'],\n",
    "        'lookback': data['lookback'].item(),\n",
    "        'padding_method': str(data['padding_method'])\n",
    "    }\n",
    "\n",
    "# 从字典创建数据集\n",
    "def create_dataset_from_dict(dataset_dict, model):\n",
    "    \"\"\"从字典创建optDataset实例，并载入预计算的sols和objs\"\"\"\n",
    "    dataset = optDataset(\n",
    "        model=model,\n",
    "        feats=dataset_dict['feats'],\n",
    "        costs=dataset_dict['costs'],\n",
    "        lookback=dataset_dict['lookback'],\n",
    "        padding_method=dataset_dict['padding_method'],\n",
    "        precomputed =True\n",
    "    )\n",
    "    \n",
    "    # 用预计算的解替换\n",
    "    dataset.sols = dataset_dict['sols']\n",
    "    dataset.objs = dataset_dict['objs']\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "# 之后使用时，可以直接加载而无需重新计算\n",
    "loaded_dict = load_dataset_dict('large_market_neutral_dataset.npz')\n",
    "dataset = create_dataset_from_dict(loaded_dict, market_neutral_model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (131010, 335, 5, 6)\n",
      "Training size: 80\n",
      "Test size: 20\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# 获取索引并进行分割\n",
    "indices = list(range(100))\n",
    "split_point = int(len(indices) * 0.8)\n",
    "train_indices = indices[:split_point]\n",
    "test_indices = indices[split_point:]\n",
    "\n",
    "# 创建 Subset 类\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "dataset_train = Subset(dataset, train_indices)\n",
    "dataset_test = Subset(dataset, test_indices)\n",
    "\n",
    "# 现在 dataset_train 和 dataset_test 都是 optDataset 对象\n",
    "print(f\"Dataset shape: {dataset_train.dataset.feats.shape}\")\n",
    "print(f\"Training size: {len(dataset_train)}\")\n",
    "\n",
    "print(f\"Test size: {len(dataset_test)}\")\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "# DATA LOADERS WITH PREFETCHING\n",
    "#############################################################################\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "# Find optimal batch size based on GPU memory\n",
    "# Start with a reasonable default\n",
    "batch_size = 8  \n",
    "\n",
    "generator = torch.Generator()\n",
    "generator.manual_seed(42)\n",
    "\n",
    "# Configure data loaders with prefetching\n",
    "loader_train = DataLoader(\n",
    "    dataset_train, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    generator=generator # 固定data loader的生成器 - 用于对比MLP & MLP_Forloop\n",
    "    # pin_memory=True,  # Speed up host to GPU transfers\n",
    "    # num_workers=2,    # Prefetch in parallel\n",
    "    # persistent_workers=True  # Keep workers alive between epochs\n",
    ")\n",
    "\n",
    "loader_test = DataLoader(\n",
    "    dataset_test, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,\n",
    "    # pin_memory=True,\n",
    "    # num_workers=1\n",
    ")\n",
    "\n",
    "# move loader to device for logging regret\n",
    "def device_loader(loader, device):\n",
    "    for batch in loader:\n",
    "        x, c, w, z = batch\n",
    "        yield x.to(device), c.to(device), w.to(device), z.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing SPO+ loss function...\n",
      "Num of cores: 1\n"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "# TRAINING WITH MIXED PRECISION\n",
    "#############################################################################\n",
    "import time\n",
    "from torch.amp import GradScaler, autocast\n",
    "import pyepo\n",
    "\n",
    "def trainModel(model, loss_func, method_name, num_epochs=5, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Enhanced training function with:\n",
    "    - Mixed precision for faster GPU training\n",
    "    - Learning rate scheduling\n",
    "    - Progress bars\n",
    "    - Detailed logging\n",
    "    - Memory-efficient tensor handling\n",
    "    \"\"\"\n",
    "    # Set up optimizer with weight decay for regularization\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    \n",
    "    # Learning rate scheduler for better convergence\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=1\n",
    "    )\n",
    "    \n",
    "    # Enable mixed precision training\n",
    "    scaler = GradScaler(enabled=(device.type in [\"cuda\", \"mps\"]))\n",
    "\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Initialize logs\n",
    "    loss_log = []\n",
    "    # evaluate loss on whole test data\n",
    "    loss_log_regret = [ sequential_regret(model, market_neutral_model_testing, device_loader(loader_test, device))]\n",
    "    print(f\"Initial regret: {loss_log_regret[0]*100:.4f}%\")\n",
    "    \n",
    "    # Initialize elapsed time tracking\n",
    "    training_start = time.time()\n",
    "    total_elapsed = 0\n",
    "    \n",
    "    # Verbosity control - set to false for production\n",
    "    debug_mode = False\n",
    "    log_interval = 10  # Log every N batches\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        # Progress bar for this epoch\n",
    "        progress_bar = tqdm(loader_train, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for i, data in enumerate(progress_bar):\n",
    "            x, c, w, z = data\n",
    "            \n",
    "            # Move data to GPU (once, not in every batch)\n",
    "            x, c, w, z = x.to(device), c.to(device), w.to(device), z.to(device)\n",
    "            \n",
    "            # Record batch start time for accurate timing\n",
    "            batch_start = time.time()\n",
    "            \n",
    "            # Clear gradients for each batch\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Use mixed precision where appropriate\n",
    "            with autocast(device_type=device.type, enabled=(device.type in [\"cuda\", \"mps\"])):\n",
    "                # Forward pass\n",
    "                cp = model(x)\n",
    "                \n",
    "                # Compute loss based on method\n",
    "                if method_name == \"spo+\":\n",
    "                    loss = loss_func(cp, c, w, z)\n",
    "                elif method_name in [\"ptb\", \"pfy\", \"imle\", \"aimle\", \"nce\", \"cmap\"]:\n",
    "                    loss = loss_func(cp, w)\n",
    "                elif method_name in [\"dbb\", \"nid\"]:\n",
    "                    loss = loss_func(cp, c, z)\n",
    "                elif method_name in [\"pg\", \"ltr\"]:\n",
    "                    loss = loss_func(cp, c)\n",
    "            \n",
    "            # Backward pass with mixed precision handling\n",
    "            if scaler is not None:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Track batch elapsed time\n",
    "            batch_elapsed = time.time() - batch_start\n",
    "            total_elapsed += batch_elapsed\n",
    "            \n",
    "            # Update loss tracking\n",
    "            current_loss = loss.item()\n",
    "            epoch_loss += current_loss\n",
    "            loss_log.append(current_loss)\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f\"{current_loss:.4f}\", \n",
    "                'batch time': f\"{batch_elapsed:.4f}s\"\n",
    "            })\n",
    "            \n",
    "            # Debug logging (limited to avoid overwhelming output)\n",
    "            if debug_mode and i % log_interval == 0:\n",
    "                print(f\"\\n[Debug] Batch {i} stats:\")\n",
    "                print(f\"Loss: {current_loss:.6f}\")\n",
    "                print(f\"Pred shape: {cp.shape}, values: {cp[0,:5].detach().cpu().numpy()}\")\n",
    "                \n",
    "                # Monitor memory usage\n",
    "                if device.type == 'cuda':\n",
    "                    mem_allocated = torch.cuda.memory_allocated() / 1024**2\n",
    "                    mem_reserved = torch.cuda.memory_reserved() / 1024**2\n",
    "                    print(f\"GPU Memory: {mem_allocated:.1f}MB allocated, {mem_reserved:.1f}MB reserved\")\n",
    "        \n",
    "        # Compute regret on test set after each epoch\n",
    "        with torch.no_grad():\n",
    "            model.eval()  # Set model to evaluation mode\n",
    "            regret = sequential_regret(model, market_neutral_model_testing, device_loader(loader_test, device))\n",
    "            model.train()  # Set back to training mode\n",
    "            loss_log_regret.append(regret)\n",
    "        \n",
    "        # Update learning rate scheduler\n",
    "        scheduler.step(epoch_loss)\n",
    "        \n",
    "        # End of epoch reporting\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        print(f\"Epoch {epoch+1}: Loss={epoch_loss/len(loader_train):.6f}, \"\n",
    "              f\"Regret={regret*100:.4f}%, Time={epoch_time:.2f}s\")\n",
    "    \n",
    "    # Report total training time\n",
    "    total_training_time = time.time() - training_start\n",
    "    print(f\"Total training time: {total_training_time:.2f}s, \"\n",
    "          f\"Effective computation time: {total_elapsed:.2f}s\")\n",
    "    \n",
    "    return loss_log, loss_log_regret\n",
    "\n",
    "#############################################################################\n",
    "# TRAINING EXECUTION \n",
    "#############################################################################\n",
    "print(\"\\nInitializing SPO+ loss function...\")\n",
    "spop = pyepo.func.SPOPlus(market_neutral_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Using Apple Silicon GPU via Metal Performance Shaders (MPS)\n"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "# IMPORTS AND SETUP\n",
    "#############################################################################\n",
    "# Set random seed for reproducibility\n",
    "import random\n",
    "random.seed(42)\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import torch\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True  # Makes training more deterministic\n",
    "torch.backends.cudnn.benchmark = False     # Can speed up training when input sizes don't change\n",
    "\n",
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import os\n",
    "\n",
    "from tqdm.auto import tqdm  # Progress bar\n",
    "\n",
    "\n",
    "# Check and set up GPU (support MPS on Mac)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Print extra info\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    torch.cuda.empty_cache()\n",
    "elif device.type == \"mps\":\n",
    "    print(\"Using Apple Silicon GPU via Metal Performance Shaders (MPS)\")\n",
    "\n",
    "\n",
    "# ##### 强制使用CPU\n",
    "# # os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "# device = torch.device(\"cpu\")\n",
    "# # Force JAX to use the CPU backend only (avoids CUDA OOM in JAX)\n",
    "# os.environ['JAX_PLATFORMS'] = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting model training...\n",
      "Initial regret: 364.1340%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a81e93332dee455f90c93ff99c766cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Size of cost vector cannot match vars.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m lstm_efficient \u001b[38;5;241m=\u001b[39m TwoLayerLSTM(k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, lstm_hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, dropout_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting model training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m loss_log, loss_log_regret \u001b[38;5;241m=\u001b[39m \u001b[43mtrainModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlstm_efficient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspo+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Increased for better convergence\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Adjusted learning rate\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 71\u001b[0m, in \u001b[0;36mtrainModel\u001b[0;34m(model, loss_func, method_name, num_epochs, lr)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Compute loss based on method\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspo+\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 71\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mptb\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpfy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimle\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maimle\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnce\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcmap\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     73\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_func(cp, w)\n",
      "File \u001b[0;32m~/Library/r-miniconda-arm64/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/r-miniconda-arm64/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/vscodefiles/paper_project/zyh/pyEPO/my-PyEPO/pkg/pyepo/func/surrogate.py:47\u001b[0m, in \u001b[0;36mSPOPlus.forward\u001b[0;34m(self, pred_cost, true_cost, true_sol, true_obj)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, pred_cost, true_cost, true_sol, true_obj):\n\u001b[1;32m     44\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m    Forward pass\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_cost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_cost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_sol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# reduction\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Library/r-miniconda-arm64/lib/python3.10/site-packages/torch/autograd/function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/vscodefiles/paper_project/zyh/pyEPO/my-PyEPO/pkg/pyepo/func/surrogate.py:90\u001b[0m, in \u001b[0;36mSPOPlusFunc.forward\u001b[0;34m(ctx, pred_cost, true_cost, true_sol, true_obj, module)\u001b[0m\n\u001b[1;32m     86\u001b[0m z \u001b[38;5;241m=\u001b[39m true_obj\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# check sol\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m#_check_sol(c, w, z)\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# solve\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m sol, obj \u001b[38;5;241m=\u001b[39m \u001b[43m_solve_or_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# calculate loss\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39moptmodel\u001b[38;5;241m.\u001b[39mmodelSense \u001b[38;5;241m==\u001b[39m EPO\u001b[38;5;241m.\u001b[39mMINIMIZE:\n",
      "File \u001b[0;32m~/Documents/vscodefiles/paper_project/zyh/pyEPO/my-PyEPO/pkg/pyepo/func/utlis.py:28\u001b[0m, in \u001b[0;36m_solve_or_cache\u001b[0;34m(cp, module)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# solve optimization\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39msolve_ratio:\n\u001b[0;32m---> 28\u001b[0m     sol, obj \u001b[38;5;241m=\u001b[39m \u001b[43m_solve_in_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocesses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpool\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39msolve_ratio \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;66;03m# add into solpool\u001b[39;00m\n\u001b[1;32m     31\u001b[0m         module\u001b[38;5;241m.\u001b[39m_update_solution_pool(sol)\n",
      "File \u001b[0;32m~/Documents/vscodefiles/paper_project/zyh/pyEPO/my-PyEPO/pkg/pyepo/func/utlis.py:69\u001b[0m, in \u001b[0;36m_solve_in_pass\u001b[0;34m(cp, optmodel, processes, pool)\u001b[0m\n\u001b[1;32m     66\u001b[0m obj \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(ins_num):\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# solve\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m     \u001b[43moptmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetObj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     solp, objp \u001b[38;5;241m=\u001b[39m optmodel\u001b[38;5;241m.\u001b[39msolve()\n\u001b[1;32m     71\u001b[0m     sol\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mas_tensor(solp))\n",
      "File \u001b[0;32m~/Documents/vscodefiles/paper_project/zyh/pyEPO/my-PyEPO/pkg/pyepo/model/grb/grbmodel.py:64\u001b[0m, in \u001b[0;36moptGrbModel.setObj\u001b[0;34m(self, c)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03mA method to set objective function\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03m    c (np.ndarray / list): cost of objective function\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(c) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_cost:\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize of cost vector cannot match vars.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# check if c is a PyTorch tensor\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(c, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "\u001b[0;31mValueError\u001b[0m: Size of cost vector cannot match vars."
     ]
    }
   ],
   "source": [
    "lstm_efficient = TwoLayerLSTM(k=6, hidden_dim=32, lstm_hidden_dim=64, dropout_rate=0.0).to(device)\n",
    "print(\"\\nStarting model training...\")\n",
    "loss_log, loss_log_regret = trainModel(\n",
    "    lstm_efficient, \n",
    "    loss_func=spop, \n",
    "    method_name=\"spo+\",\n",
    "    num_epochs=5,  # Increased for better convergence\n",
    "    lr=1e-3        # Adjusted learning rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Visualizing learning curves...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'loss_log' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mVisualizing learning curves...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 59\u001b[0m visLearningCurve(\u001b[43mloss_log\u001b[49m, loss_log_regret)\n\u001b[1;32m     60\u001b[0m visLearningCurve(loss_log_fl, loss_log_regret_fl)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_log' is not defined"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "# VISUALIZATION\n",
    "#############################################################################\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def visLearningCurve(loss_log, loss_log_regret):\n",
    "    \"\"\"Enhanced visualization with smoother curves and more information\"\"\"\n",
    "    # Create figure and subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,4))\n",
    "    \n",
    "    # Plot training loss with smoothing for readability\n",
    "    n_points = len(loss_log)\n",
    "    \n",
    "    # Apply smoothing for large datasets\n",
    "    if n_points > 100:\n",
    "        window_size = max(10, n_points // 50)\n",
    "        smoothed_loss = np.convolve(loss_log, np.ones(window_size)/window_size, mode='valid')\n",
    "        x_axis = np.arange(len(smoothed_loss))\n",
    "        ax1.plot(x_axis, smoothed_loss, color=\"c\", lw=2, label=f\"Smoothed (window={window_size})\")\n",
    "        # Also plot the raw data with transparency\n",
    "        ax1.plot(loss_log, color=\"c\", lw=0.5, alpha=0.3, label=\"Raw\")\n",
    "        ax1.legend()\n",
    "    else:\n",
    "        # For smaller datasets, just plot the raw data\n",
    "        ax1.plot(loss_log, color=\"c\", lw=2)\n",
    "    \n",
    "    ax1.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "    ax1.set_xlabel(\"Iterations\", fontsize=16)\n",
    "    ax1.set_ylabel(\"Loss\", fontsize=16)\n",
    "    ax1.set_title(\"Learning Curve on Training Set\", fontsize=16)\n",
    "    ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Draw plot for regret on test set\n",
    "    epochs = np.arange(len(loss_log_regret))\n",
    "    ax2.plot(epochs, [r*100 for r in loss_log_regret], \n",
    "             color=\"royalblue\", marker='o', ls=\"-\", alpha=0.8, lw=2)\n",
    "    \n",
    "    ax2.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "    ax2.set_ylim(0, max(50, max([r*100 for r in loss_log_regret])*1.1))  # Dynamic y-limit\n",
    "    ax2.set_xlabel(\"Epochs\", fontsize=16)\n",
    "    ax2.set_ylabel(\"Regret (%)\", fontsize=16)\n",
    "    ax2.set_title(\"Learning Curve on Test Set\", fontsize=16)\n",
    "    ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add values to points\n",
    "    for i, r in enumerate(loss_log_regret):\n",
    "        ax2.annotate(f\"{r*100:.2f}%\", \n",
    "                     (i, r*100),\n",
    "                     textcoords=\"offset points\", \n",
    "                     xytext=(0,10), \n",
    "                     ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('learning_curves.png', dpi=300, bbox_inches='tight')\n",
    "    # print(\"Saved learning curves to 'learning_curves.png'\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nVisualizing learning curves...\")\n",
    "visLearningCurve(loss_log, loss_log_regret)\n",
    "visLearningCurve(loss_log_fl, loss_log_regret_fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
